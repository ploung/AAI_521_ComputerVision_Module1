{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "55295c49",
   "metadata": {},
   "source": [
    "## Hand writing image recognition using ANN in OpenCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aaafd366",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "import cv2\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1181879e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gzip\n",
    "import pickle\n",
    "\n",
    "def load_data():\n",
    "    print(\"Loading MNIST database...\")\n",
    "    with gzip.open('mnist.pkl.gz', 'rb') as file:\n",
    "        #(train_images, training_ids), (test_images, tests_ids) = pickle.load(file, encoding='latin1')\n",
    "        train_images, test_images = pickle.load(file, encoding='latin1')\n",
    "    return (train_images, test_images)\n",
    "\n",
    "def vectorized_result(j):\n",
    "    e = np.zeros((10, 1))\n",
    "    e[j] = 1.0\n",
    "    return e\n",
    "\n",
    "def reformat_data():\n",
    "    tr_d, te_d = load_data_enhanced()  # Use enhanced version\n",
    "    training_inputs = tr_d[0]\n",
    "    training_results = [vectorized_result(y) for y in tr_d[1]]\n",
    "    training_data = zip(training_inputs, training_results)\n",
    "    \n",
    "    test_inputs = te_d[0]\n",
    "    test_data = zip(test_inputs, te_d[1])\n",
    "    \n",
    "    return (training_data, test_data)\n",
    "\n",
    "# Data: tuple containing training and test data of features and labels\n",
    "# samples: number of samples to use for training\n",
    "# epochs: number of iterations to use for training\n",
    "def train_ann(ann, samples=50000, epochs=10):\n",
    "    print(\"Training ANN model...\")\n",
    "    training_data, test_data = reformat_data()\n",
    "    \n",
    "    # Convert iterable to list and limit to 'samples' number of samples\n",
    "    training_data = list(training_data)[:samples]\n",
    "    \n",
    "    training_inputs = [np.array(x).reshape(-1, 1) for x, y in training_data]\n",
    "    training_outputs = [y for x, y in training_data]\n",
    "    \n",
    "    training_inputs = np.array(training_inputs).reshape(len(training_inputs), -1)\n",
    "    training_outputs = np.array(training_outputs).reshape(len(training_outputs), -1)\n",
    "    \n",
    "    ann.train(cv2.ml.TrainData_create(training_inputs.astype(np.float32), \n",
    "                                     cv2.ml.ROW_SAMPLE, \n",
    "                                     training_outputs.astype(np.float32)))\n",
    "    \n",
    "    return ann, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f4dcb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative method using requests library\n",
    "import requests\n",
    "import io\n",
    "\n",
    "def load_data_from_url_direct():\n",
    "    \"\"\"Load MNIST data directly from URL without saving to disk\"\"\"\n",
    "    print(f\"Downloading and loading MNIST from {url}...\")\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()\n",
    "    \n",
    "    # Load directly from memory\n",
    "    with gzip.open(io.BytesIO(response.content), 'rb') as file:\n",
    "        (train_images, train_labels), (test_images, test_labels) = pickle.load(file, encoding='latin1')\n",
    "    \n",
    "    print(\"Data loaded successfully!\")\n",
    "    return (train_images, train_labels), (test_images, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7e132c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method with progress bar (install tqdm if needed: pip install tqdm)\n",
    "from tqdm import tqdm\n",
    "\n",
    "def download_with_progress(url, filename):\n",
    "    \"\"\"Download file with progress bar\"\"\"\n",
    "    if os.path.exists(filename):\n",
    "        print(f\"{filename} already exists.\")\n",
    "        return\n",
    "    \n",
    "    print(f\"Downloading {filename}...\")\n",
    "    response = requests.get(url, stream=True)\n",
    "    total_size = int(response.headers.get('content-length', 0))\n",
    "    \n",
    "    with open(filename, 'wb') as file, tqdm(\n",
    "        desc=filename,\n",
    "        total=total_size,\n",
    "        unit='B',\n",
    "        unit_scale=True,\n",
    "        unit_divisor=1024,\n",
    "    ) as pbar:\n",
    "        for chunk in response.iter_content(chunk_size=8192):\n",
    "            if chunk:\n",
    "                file.write(chunk)\n",
    "                pbar.update(len(chunk))\n",
    "    print(\"Download completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e563302b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create and configure the Artificial Neural Network model\n",
    "def create_ann(hidden_nodes=60): # Caller can specify number of hidden nodes, but default is 60\n",
    "    ann = cv2.ml.ANN_MLP_create()\n",
    "    ann.setLayerSizes(np.array([784, hidden_nodes, 10])) # Input layer: 784 nodes, Hidden layer: hidden_nodes, Output layer: 10 nodes\n",
    "    ann.setActivationFunction(cv2.ml.ANN_MLP_SIGMOID_SYM, 0.6, 1.0)\n",
    "    ann.setTrainMethod(cv2.ml.ANN_MLP_BACKPROP, 0.1, 0.1)\n",
    "    ann.setTermCriteria((cv2.TERM_CRITERIA_MAX_ITER | cv2.TERM_CRITERIA_EPS, 100, 1.0))\n",
    "    return ann"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28bc06b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training function and train the ANN model\n",
    "def train_ann(ann, samples=50000, epochs=10):\n",
    "    print(\"Training ANN model...\")\n",
    "    training_data, test_data = reformat_data()\n",
    "\n",
    "    # Convert iterable to list and limit to 'samples' number of samples\n",
    "    training_data = list(training_data)[:samples]\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\" Completed %d/%d epochs\" % (epoch, epochs))\n",
    "        counter = 0\n",
    "        for image in training_data:\n",
    "            if (counter > samples):\n",
    "                break\n",
    "            if (counter % 1000 == 0):\n",
    "                print(f\"Epoch %d: Trained on %d/%d samples\" % (epoch, counter, len(training_data)))\n",
    "            counter += 1\n",
    "        \n",
    "            sample, response = image\n",
    "            data = cv2.ml.TrainData_create(\n",
    "                np.array([sample], np.float32),\n",
    "                cv2.ml.ROW_SAMPLE,\n",
    "                np.array([response], np.float32)\n",
    "            )\n",
    "\n",
    "            if ann.isTrained():\n",
    "                ann.train(data, cv2.ml.ANN_MLP_UPDATE_WEIGHTS | cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
    "            else:\n",
    "                ann.train(data, cv2.ml.ANN_MLP_NO_INPUT_SCALE | cv2.ml.ANN_MLP_NO_OUTPUT_SCALE)\n",
    "        return (ann, test_data)\n",
    "    print(\"Training completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e8a20dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create prediction function to evaluate the ANN model\n",
    "from random import sample\n",
    "\n",
    "\n",
    "def predict_ann(ann, test_data):\n",
    "    if test_data.shape != (784, ):\n",
    "        if test_data.shape != (28, 28):\n",
    "            interpolation = cv2.INTER_LINEAR\n",
    "        test_data = sample.reshape(784, )\n",
    "    return ann.predict(np.array([test_data], np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1cdaead4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create test function to evaluate the ANN model\n",
    "def test_ann(ann, test_data):\n",
    "    print(\"Testing ANN model...\")\n",
    "    num_tests = 0\n",
    "    number_correct = 0\n",
    "\n",
    "    for image, label in test_data:\n",
    "        num_tests += 1\n",
    "        result = predict_ann(ann, image)\n",
    "        predicted_label = np.argmax(result[0])\n",
    "        if predicted_label == label:\n",
    "            number_correct += 1\n",
    "\n",
    "    accuracy = number_correct / num_tests\n",
    "    print(f\"Accuracy: {accuracy * 100:.2f}% ({number_correct}/{num_tests})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a2a04793",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating synthetic data for testing OpenCV ANN...\n",
      "Training data shape: (800, 784)\n",
      "Training labels shape: (800,)\n",
      "Test data shape: (200, 784)\n",
      "Test labels shape: (200,)\n",
      "\n",
      "Training OpenCV ANN with synthetic data...\n",
      "Training completed!\n",
      "\n",
      "Testing the network...\n",
      "Test accuracy with synthetic data: 11.00%\n",
      "\n",
      "Note: This is synthetic random data, so accuracy will be around 10% (random chance)\n",
      "Once MNIST data is available, accuracy should be much higher!\n"
     ]
    }
   ],
   "source": [
    "# Test with synthetic data since download is timing out\n",
    "print(\"Creating synthetic data for testing OpenCV ANN...\")\n",
    "\n",
    "# Create synthetic MNIST-like data for testing\n",
    "def create_synthetic_data(n_samples=1000):\n",
    "    \"\"\"Create synthetic data similar to MNIST format\"\"\"\n",
    "    np.random.seed(42)  # For reproducibility\n",
    "    \n",
    "    # Create random 28x28 images (flattened to 784 features)\n",
    "    images = np.random.rand(n_samples, 784).astype(np.float32)\n",
    "    \n",
    "    # Create random labels (0-9)\n",
    "    labels = np.random.randint(0, 10, n_samples)\n",
    "    \n",
    "    # Split into train/test (80/20)\n",
    "    split = int(0.8 * n_samples)\n",
    "    train_images = images[:split]\n",
    "    train_labels = labels[:split]\n",
    "    test_images = images[split:]\n",
    "    test_labels = labels[split:]\n",
    "    \n",
    "    return (train_images, train_labels), (test_images, test_labels)\n",
    "\n",
    "# Create synthetic data\n",
    "(train_data, train_labels), (test_data, test_labels) = create_synthetic_data(1000)\n",
    "\n",
    "print(f\"Training data shape: {train_data.shape}\")\n",
    "print(f\"Training labels shape: {train_labels.shape}\")\n",
    "print(f\"Test data shape: {test_data.shape}\")\n",
    "print(f\"Test labels shape: {test_labels.shape}\")\n",
    "\n",
    "# Test the OpenCV ANN\n",
    "ann = create_ann()\n",
    "\n",
    "# Prepare labels in one-hot format for training\n",
    "train_labels_onehot = np.zeros((len(train_labels), 10), dtype=np.float32)\n",
    "for i, label in enumerate(train_labels):\n",
    "    train_labels_onehot[i, label] = 1.0\n",
    "\n",
    "print(\"\\nTraining OpenCV ANN with synthetic data...\")\n",
    "\n",
    "# Create training data for OpenCV\n",
    "train_data_cv = cv2.ml.TrainData_create(\n",
    "    train_data.astype(np.float32),\n",
    "    cv2.ml.ROW_SAMPLE,\n",
    "    train_labels_onehot.astype(np.float32)\n",
    ")\n",
    "\n",
    "# Train the network\n",
    "ann.train(train_data_cv)\n",
    "\n",
    "print(\"Training completed!\")\n",
    "\n",
    "# Test the network\n",
    "print(\"\\nTesting the network...\")\n",
    "test_predictions = ann.predict(test_data.astype(np.float32))[1]\n",
    "predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = np.mean(predicted_labels == test_labels) * 100\n",
    "print(f\"Test accuracy with synthetic data: {accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nNote: This is synthetic random data, so accuracy will be around 10% (random chance)\")\n",
    "print(\"Once MNIST data is available, accuracy should be much higher!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e997abe",
   "metadata": {},
   "source": [
    "## Alternative: Download MNIST Manually\n",
    "\n",
    "If the automatic download fails due to network issues, you can:\n",
    "\n",
    "1. **Download manually**: Go to http://deeplearning.net/data/mnist/mnist.pkl.gz and save it to your working directory\n",
    "2. **Use alternative sources**: Try downloading from other MNIST sources\n",
    "3. **Use TensorFlow's MNIST**: Load using `tf.keras.datasets.mnist.load_data()` instead\n",
    "\n",
    "Once you have the MNIST data file (`mnist.pkl.gz`), the enhanced `load_data_enhanced()` function will work perfectly!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "278d4cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Alternative: Use TensorFlow's MNIST dataset\n",
    "import tensorflow as tf\n",
    "\n",
    "def load_mnist_tensorflow():\n",
    "    \"\"\"Load MNIST using TensorFlow and format for OpenCV\"\"\"\n",
    "    print(\"Loading MNIST from TensorFlow...\")\n",
    "    (x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "    \n",
    "    # Normalize and flatten the images\n",
    "    x_train = x_train.reshape(x_train.shape[0], -1) / 255.0\n",
    "    x_test = x_test.reshape(x_test.shape[0], -1) / 255.0\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)\n",
    "\n",
    "def train_ann_tensorflow(samples=10000):\n",
    "    \"\"\"Train OpenCV ANN using TensorFlow's MNIST data\"\"\"\n",
    "    (train_data, train_labels), (test_data, test_labels) = load_mnist_tensorflow()\n",
    "    \n",
    "    # Limit training samples\n",
    "    train_data = train_data[:samples]\n",
    "    train_labels = train_labels[:samples]\n",
    "    \n",
    "    # Convert labels to one-hot encoding\n",
    "    train_labels_onehot = np.zeros((len(train_labels), 10), dtype=np.float32)\n",
    "    for i, label in enumerate(train_labels):\n",
    "        train_labels_onehot[i, label] = 1.0\n",
    "    \n",
    "    # Create and train ANN\n",
    "    ann = create_ann()\n",
    "    train_data_cv = cv2.ml.TrainData_create(\n",
    "        train_data.astype(np.float32),\n",
    "        cv2.ml.ROW_SAMPLE,\n",
    "        train_labels_onehot\n",
    "    )\n",
    "    \n",
    "    print(f\"Training with {len(train_data)} samples...\")\n",
    "    ann.train(train_data_cv)\n",
    "    \n",
    "    # Test the network\n",
    "    print(\"Testing on real MNIST data...\")\n",
    "    test_predictions = ann.predict(test_data[:1000].astype(np.float32))[1]\n",
    "    predicted_labels = np.argmax(test_predictions, axis=1)\n",
    "    \n",
    "    accuracy = np.mean(predicted_labels == test_labels[:1000]) * 100\n",
    "    print(f\"MNIST Test Accuracy: {accuracy:.2f}%\")\n",
    "    \n",
    "    return ann\n",
    "\n",
    "# Uncomment to run with TensorFlow MNIST:\n",
    "# ann_tf = train_ann_tensorflow(samples=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2df05b8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing OpenCV ANN with real MNIST data from TensorFlow...\n",
      "Loading MNIST from TensorFlow...\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "\u001b[1m11490434/11490434\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n",
      "Training with 5000 samples...\n",
      "Testing on real MNIST data...\n",
      "MNIST Test Accuracy: 90.40%\n"
     ]
    }
   ],
   "source": [
    "# Test with TensorFlow MNIST (uncommented)\n",
    "print(\"Testing OpenCV ANN with real MNIST data from TensorFlow...\")\n",
    "ann_tf = train_ann_tensorflow(samples=5000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
